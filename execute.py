# -*- coding: utf-8 -*-
"""execute.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-ClGx7Akys4O3ZibOc2MdfdimbE8fL9t
"""

import pip

def pip_install(package):
    if hasattr(pip, 'main'):
        pip.main(['install', package])
    else:
        pip._internal.main(['install', package])

pip_install('psycopg2-binary')

pip_install('SQLAlchemy')

# DB
import psycopg2
import pandas.io.sql as psql
import warnings

# config
import configparser

# sup
import os
import pandas as pd
import re
from datetime import datetime

def timedelta_to_time(t):
    """
    timedelta 타입을 string 타입으로 변환.
    ex) "3532.125243 secs", "1 days 142.132423 secs"
    """
    days = t.days
    seconds = t.seconds
    microseconds = t.microseconds

    if days > 0:
        time = str(days) + " days "+ str(seconds) + "." + str(microseconds) + " secs"
    else:
        time = str(seconds) + "." + str(microseconds) + " secs"
    
    return time

def get_today():
    now = datetime.now()
    date = now.date()
    date = str(date).replace("-","_")
    return date

def createDirectory(directory):
    try:
        if not os.path.exists(directory):
            os.makedirs(directory)
    except OSError:
        print("Error: Failed to create the directory.")

def connect(host, dbname, user, password, port):
    connection = psycopg2.connect(host=host, dbname = dbname, user = user, password = password, port = port)
    return connection

def read_sql(sql, connection):
    result = psql.read_sql(sql, connection)
    return result

def fill_result_table(result, execution_time, check):
    result.insert(len(result.columns), 'EXECUTION_TIME', execution_time)
    result.insert(len(result.columns), 'QUERY_TEXT', check['query_text'])
    result.insert(len(result.columns), 'CHECK_NAME', check['checkName'])
    result.insert(len(result.columns), 'CHECK_DESCRIPTION', check['checkDescription'])
    result.insert(len(result.columns), 'CDM_TABLE_NAME', check['cdmTableName'])
    result.insert(len(result.columns), 'SQL_FILE', check['sqlFile'])
    result.insert(len(result.columns), 'checkId', check['checkId'])

def run_checks(checks, connection, output_folder):
    #ignore warnings
    warnings.filterwarnings(action='ignore')

    # directory
    result_dir = output_folder
    query_result_dir = os.path.join(result_dir, "query_result")
    dqd_result_dir = result_dir

    # make initial dqd result table from query result
    dqd_result = pd.DataFrame()
    entire_start_time = datetime.now()
    result_columns = list()

    # createDirectory
    createDirectory(query_result_dir)
    createDirectory(dqd_result_dir)


    for i in range(len(checks)):
        check = checks[i]
        file_exist = os.path.exists(os.path.join(output_folder,check['checkId']+".csv"))

        if not file_exist:
            # process start
            start_time = datetime.now()
            print(i,":",check['checkId'],"CHECK is started. ---- Start_time:", start_time)

            # query_execution
            result = read_sql(check['query_text'], connection)

            # execution time
            end_time = datetime.now()
            execution_time = timedelta_to_time(end_time - start_time) 
            print(i,":",check['checkId'],"CHECK is done. ---- Execution time:", execution_time)

            # additional information to the table
            fill_result_table(result, execution_time, check)

            # save query result
            filename = check['checkId']+".csv"
            result.to_csv(os.path.join(query_result_dir,filename), encoding='EUC_KR', index=False)

        if file_exist:
            # already checked (read query result from csv)
            print(i,":",check['checkId'],"CHECK is already done.")
            result = pd.read_csv(os.path.join(query_result_dir,check['checkId']+".csv"), encoding = "EUC_KR")


        ### result table
    
        # write to a dqd result table
        dqd_result = pd.concat([dqd_result, result], ignore_index=True)

        # append to a new_columns list
        temp_columns = list(result.columns)
        for c in temp_columns:
          if c not in result_columns:
            result_columns.append(c)

        # (change the order of the columns)
        old_columns = list(dqd_result.columns)
        new_columns = result_columns.copy()
        for c in old_columns:
          if c not in new_columns:
            new_columns.append(c)
        
        # change
        dqd_result = dqd_result[new_columns]

        # save
        dqd_result_filename = "dqd_result.csv"
        dqd_result.to_csv(os.path.join(dqd_result_dir,dqd_result_filename),encoding='EUC_KR',index=False)

    # ignore warnings cancel
    warnings.filterwarnings(action='default')

def get_script(filename, directory = None):
    f = open(os.path.join(directory,filename), mode = 'rt', encoding='UTF8')
    script = f.read()
    f.close()
    return script

def convert_query(script, params):
    # script <- query including a markdown
    # params <- record(dict)
    converted = script
    
    for parameter in params:
        if pd.notna(parameter):
            converted =  converted.replace(str('@'+parameter), str(params[parameter]))
            
    return converted

def execute_subquery(subquery, column_name, host, dbname, user, password, port):
    connection = connect(host, dbname, user, password, port)
    warnings.filterwarnings(action='ignore')
    
    result = psql.read_sql(subquery, connection)
    
    warnings.filterwarnings(action='default')
    connection.close()
    
    result_list = result.to_dict('list')
    result = result_list[column_name][0]
    
    return result

def load_csv_files(csv_dir):
    files_ = os.listdir(csv_dir)
    
    

    csvs = dict()

    for f in files_:
        if '.csv' in f:
            check_df = pd.read_csv(os.path.join(csv_dir,f), encoding = "EUC_KR")
            if 'cdmTableName' in check_df.columns:
                check_df.sort_values(by=['cdmTableName'])
            csvs[f] = check_df
            print(f,"read successfully.")

    return csvs

def load_sql_files(sql_dir):
    files_ = os.listdir(sql_dir)
    scripts = dict()

    for f in files_:
        if '.sql' in f:
            script = get_script(f, sql_dir)
            scripts[f] = script
            print(f,"read successfully.")

    return scripts

def load_subquery_files(subquery_dir):
    files_ = os.listdir(subquery_dir)
    subquerys = dict()

    for f in files_:
        if '.sql' in f:
            subquery = get_script(f, subquery_dir)
            subquerys[f] = subquery
            print(f, "read successfully.")
    
    return subquerys

def make_checks(description_df, scripts, csvs, subquerys, check_dir, host, dbname, user, password, port):
    checks = list()
    description_dict = description_df.to_dict('records')

    for i in range(len(description_df)):
        row = description_dict[i]
        record = dict()

        record['checkName'] = row['checkName']
        record['checkDescription'] = row['checkDescription']
        sqlFile = row['sqlFile']
        csvFile = row['csvFile']
        script = scripts[sqlFile]
        csv_df = csvs[csvFile]
        csv_dict = csv_df.to_dict('records')
        subqueryFile = row['subQuery']
        if pd.notna(subqueryFile):
            subquery = subquerys[subqueryFile]
        
        # csv_df
        for j in range(len(csv_df)):
            params = csv_dict[j] # one record
            
            # subquery 변환
            if pd.notna(subqueryFile):
                if pd.isna(params[row['subQueryResult']]):
                    # 최초 1회 실행
                    subquery_converted = convert_query(subquery, params)
                    result = execute_subquery(subquery_converted, row['subQueryResult'], host, dbname, user, password, port)
                    params[row['subQueryResult']] = result
                    
                    csv_df = pd.DataFrame(csv_dict)
                    csv_df.to_csv(os.path.join(csv_dir, csvFile), encoding = 'EUC_KR', index=False)
            
            # sql 변환
            sql = convert_query(script, params)
            record['query_text'] = sql

            # description 변환
            description = convert_query(row['checkDescription'] ,params)
            record['checkDescription'] = description
            
            record['cdmTableName'] = str(params['cdmTableName'])
            record['sqlFile'] = sqlFile
            
            checkId = str(row['checkName']) + "_" + str(params['cdmTableName'])
            if 'ConceptId' in params:
                checkId = checkId+"_"+str(params['ConceptId'])
            record['checkId'] = checkId

            
            
            
            checks.append(record.copy())

            checks_df = pd.DataFrame(checks)
            checks_filename = "checks.csv"
            checks_df.to_csv(os.path.join(check_dir, checks_filename), encoding = 'EUC_KR', index=False)

    return checks

# config 파일 읽기
config = configparser.ConfigParser()
config.read(os.path.join(os.getcwd(),'config.ini'), encoding = 'utf-8')

# config 적용
host = config['dbinfo']['host']
dbname = config['dbinfo']['dbname']
port = config['dbinfo']['port']
user = config['dbinfo']['user']
password = config['dbinfo']['password']

dqd_dir = config['directory']['dqd_dir']
output_folder = config['directory']['outputFolder']

# csv 파일 불러오기 (dataframe)
csv_dir = os.path.join(dqd_dir, "csv")
csvs = load_csv_files(csv_dir)

# sql 파일 불러오기 (script string)
sql_dir = os.path.join(dqd_dir, "sql")
scripts = load_sql_files(sql_dir)

# subquery 불러오기
subquery_dir = os.path.join(dqd_dir, "subquery")
subquerys = load_subquery_files(subquery_dir)

# description 파일 불러오기
description_df = csvs['check_description.csv']

# checks 만들기
check_dir = os.path.join(output_folder, "check")
createDirectory(check_dir)

print("checks 생성 시작")
checks = make_checks(description_df, scripts, csvs, subquerys, check_dir, host, dbname, user, password, port)
print("checks 생성 완료")

# DB 연결
print("DB 연결을 시도합니다.")
connection = connect(host, dbname, user, password, port)
print("DB 연결됨.")

# run
run_checks(checks, connection, output_folder)

# 연결 종료
connection.close()
print("DB 연결 종료됨.")